{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Opportunistic Example\n",
    "\n",
    "SGD Example our goal is:\n",
    "\n",
    "- Show the power of the opportunistic optimizer\n",
    "\n",
    "![SGD PLAN](files/sgd_plan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Preparing dependancies\n",
    "\n",
    "The coursierapi is necessary at import time because it is mandatory to include the maven repository from the local folder, this step will not be required after the release, but it will be needed if you want to modify the code and use the modified version.\n",
    "\n",
    "The piece of code below, add the location of the new MavenRepository that is located in the path **/maven/local/repository**; this is the same path use in the container stating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step imports the required modules to execute the code. All these packages come from the previous Maven Instalation\n",
    "\n",
    "The imported libraries are:\n",
    "\n",
    "Module | Java's | Scala's | Description\n",
    ":----- | -------------: | --------------: | :----------\n",
    "wayang-core | 8, 11 | 2.11, 2.12 | provides core data structures and the optimizer (required)\n",
    "wayang-basic | 8, 11 | 2.11, 2.12 | provides common operators and data types for your apps (recommended)\n",
    "wayang-api-scala-java | 8, 11 | 2.11, 2.12 | provides an easy-to-use Scala and Java API to assemble wayang plans (recommended)\n",
    "wayang-java | 8, 11 | 2.11, 2.12 | adapters for [Java Stream](https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html) processing platforms\n",
    "wayang-spark | 8, 11 | 2.11, 2.12 | adapters for [Apache Spark](https://spark.apache.org) processing platforms\n",
    "wayang-flink | 8, 11 | 2.11, 2.12 | adapters for [Apache Flink](https://flink.apache.org) processing platforms\n",
    "hadoop-common | 8,11 | - | Hadoop-commons is required because the lack of the Environment Variable **HADOOP_HOME**\n",
    "log4j-core | 8,11 | - | Logggin library to manipulate the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                        \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                           \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.api._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.api.Configuration\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.api.WayangContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.function.ExecutionContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.function.FunctionDescriptor\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.plugin.Plugin\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.util.{Tuple => WayangTuple, WayangCollections}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.java.Java\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.spark.Spark\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.File\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.ArrayList\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Arrays\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.{Collection => JavaCollection}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.List\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.math.{exp, abs, max}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConversions._\n",
       "\n",
       "//Logging change the level to INFO\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.logging.log4j.Level\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.logging.log4j.core.config.Configurator\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.wayang::wayang-api-scala-java:0.6.1-SNAPSHOT`\n",
    "import $ivy.`org.apache.wayang:wayang-core:0.6.1-SNAPSHOT`\n",
    "import $ivy.`org.apache.wayang:wayang-basic:0.6.1-SNAPSHOT`\n",
    "import $ivy.`org.apache.wayang:wayang-java:0.6.1-SNAPSHOT`\n",
    "import $ivy.`org.apache.wayang::wayang-spark:0.6.1-SNAPSHOT`\n",
    "import $ivy.`org.apache.hadoop:hadoop-common:2.8.5`\n",
    "import $ivy.`org.apache.logging.log4j:log4j-core:2.14.0`\n",
    "\n",
    "import org.apache.wayang.api._\n",
    "import org.apache.wayang.core.api.Configuration\n",
    "import org.apache.wayang.core.api.WayangContext\n",
    "import org.apache.wayang.core.function.ExecutionContext\n",
    "import org.apache.wayang.core.function.FunctionDescriptor\n",
    "import org.apache.wayang.core.plugin.Plugin\n",
    "import org.apache.wayang.core.util.{Tuple => WayangTuple, WayangCollections}\n",
    "import org.apache.wayang.java.Java\n",
    "import org.apache.wayang.spark.Spark\n",
    "import java.io.File\n",
    "import java.util.ArrayList\n",
    "import java.util.Arrays\n",
    "import java.util.{Collection => JavaCollection}\n",
    "import java.util.List\n",
    "import scala.math.{exp, abs, max}\n",
    "import scala.collection.JavaConversions._\n",
    "\n",
    "//Logging change the level to INFO\n",
    "import org.apache.logging.log4j.Level\n",
    "import org.apache.logging.log4j.core.config.Configurator\n",
    "\n",
    "Configurator.setRootLevel(Level.INFO);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here we include all the classes that has been used in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mTransform\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transform(var features: Int) \n",
    "    extends FunctionDescriptor.SerializableFunction[String, Array[Double]] {\n",
    "\n",
    "  override def apply(line: String): Array[Double] = {\n",
    "    val pointStr: Array[String] = line.split(\",\")\n",
    "    val point: Array[Double] = Array.ofDim[Double](features + 1)\n",
    "    for (i <- 0 until pointStr.length) {\n",
    "      point(i) = pointStr(i).toDouble\n",
    "    }\n",
    "    point\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mComputeLogisticGradient\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ComputeLogisticGradient\n",
    "    extends FunctionDescriptor.ExtendedSerializableFunction[Array[Double], Array[Double]] {\n",
    "\n",
    "  var weights: Array[Double] = _\n",
    "\n",
    "  override def apply(point: Array[Double]): Array[Double] = {\n",
    "    val gradient: Array[Double] = Array.ofDim[Double](point.length)\n",
    "    var dot: Double = 0\n",
    "    for (j <- 0 until weights.length) dot += weights(j) * point(j + 1)\n",
    "    for (j <- 0 until weights.length)\n",
    "      gradient(j + 1) = ((1 / (1 + exp(-1 * dot))) - point(0)) * point(j + 1)\n",
    "    //counter for the step size required in the update\n",
    "    gradient(0) = 1\n",
    "    gradient\n",
    "  }\n",
    "\n",
    "  override def open(executionContext: ExecutionContext): Unit = {\n",
    "    this.weights = executionContext\n",
    "      .getBroadcast(\"weights\")\n",
    "      .iterator()\n",
    "      .next()\n",
    "      .asInstanceOf[Array[Double]]\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mSum\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sum\n",
    "    extends FunctionDescriptor.SerializableBinaryOperator[Array[Double]] {\n",
    "\n",
    "  override def apply(o: Array[Double], o2: Array[Double]): Array[Double] = {\n",
    "    val g1: Array[Double] = o\n",
    "    val g2: Array[Double] = o2\n",
    "    if (//samples came from one partition only\n",
    "        g2 == null) g1\n",
    "    if (//samples came from one partition only\n",
    "        g1 == null) g2\n",
    "    val sum: Array[Double] = Array.ofDim[Double](g1.length)\n",
    "    \n",
    "    //count\n",
    "    sum(0) = g1(0) + g2(0)\n",
    "    for (i <- 1 until g1.length) sum(i) = g1(i) + g2(i)\n",
    "    sum\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mWeightsUpdate\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WeightsUpdate\n",
    "    extends FunctionDescriptor.ExtendedSerializableFunction[Array[Double], Array[Double]] {\n",
    "\n",
    "  var weights: Array[Double] = _\n",
    "\n",
    "  var current_iteration: Int = _\n",
    "\n",
    "  var stepSize: Double = 1\n",
    "\n",
    "  var regulizer: Double = 0\n",
    "\n",
    "  def this(stepSize: Double, regulizer: Double) = {\n",
    "    this()\n",
    "    this.stepSize = stepSize\n",
    "    this.regulizer = regulizer\n",
    "  }\n",
    "\n",
    "  override def apply(input: Array[Double]): Array[Double] = {\n",
    "    val count: Double = input(0)\n",
    "    val alpha: Double = (stepSize / (current_iteration + 1))\n",
    "    val newWeights: Array[Double] = Array.ofDim[Double](weights.length)\n",
    "    for (j <- 0 until weights.length) {\n",
    "      newWeights(j) = (1 - alpha * regulizer) * weights(j) - alpha * (1.0 / count) * input(\n",
    "          j + 1)\n",
    "    }\n",
    "    newWeights\n",
    "  }\n",
    "\n",
    "  override def open(executionContext: ExecutionContext): Unit = {\n",
    "    this.weights = executionContext\n",
    "      .getBroadcast(\"weights\")\n",
    "      .iterator()\n",
    "      .next()\n",
    "      .asInstanceOf[Array[Double]]\n",
    "    this.current_iteration = executionContext.getCurrentIteration\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mComputeNorm\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ComputeNorm\n",
    "    extends FunctionDescriptor.ExtendedSerializableFunction[Array[Double], (Double, Double)] {\n",
    "\n",
    "  var previousWeights: Array[Double] = _\n",
    "\n",
    "  override def apply(weights: Array[Double]): (Double, Double) = {\n",
    "    var normDiff: Double = 0.0\n",
    "    var normWeights: Double = 0.0\n",
    "    for (j <- 0 until weights.length) {\n",
    "      normDiff += abs(weights(j) - previousWeights(j))\n",
    "      normWeights += abs(weights(j))\n",
    "    }\n",
    "      \n",
    "    (normDiff, normWeights)\n",
    "  }\n",
    "\n",
    "  override def open(executionContext: ExecutionContext): Unit = {\n",
    "    this.previousWeights = executionContext\n",
    "      .getBroadcast(\"weights\")\n",
    "      .iterator()\n",
    "      .next()\n",
    "      .asInstanceOf[Array[Double]]\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mLoopCondition\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LoopCondition(var accuracy: Double, var max_iterations: Int)\n",
    "    extends FunctionDescriptor.ExtendedSerializablePredicate[JavaCollection[(Double, Double)]] {\n",
    "\n",
    "  private var current_iteration: Int = _\n",
    "\n",
    "  override def test(collection: JavaCollection[(Double, Double)]): Boolean = {\n",
    "    val input: (Double, Double) = WayangCollections.getSingle(collection)\n",
    "    println(\"Running iteration: \" + current_iteration)\n",
    "    (input._1 < accuracy * max(input._2, 1.0) || current_iteration > max_iterations)\n",
    "  }\n",
    "\n",
    "  override def open(executionContext: ExecutionContext): Unit = {\n",
    "    this.current_iteration = executionContext.getCurrentIteration\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mSGDImpl\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    "  * This class executes a stochastic gradient descent optimization on Rheem.\n",
    "  */\n",
    "class SGDImpl(plugins: Array[Plugin]) {\n",
    "\n",
    "  def apply(confFile: Configuration, datasetUrl: String, datasetSize: Int, features: Int, maxIterations: Int, accuracy: Double, sampleSize: Int): Array[Double] = {\n",
    "    // Initialize the builder.\n",
    "    val context = new WayangContext(confFile)\n",
    "    for (plugin <- this.plugins) {\n",
    "      context.withPlugin(plugin)\n",
    "    }\n",
    "    val planBuilder = new PlanBuilder(context)\n",
    "      \n",
    "    // Create initial weights.\n",
    "    val weights: List[Array[Double]] = Arrays.asList(Array.ofDim[Double](features))\n",
    "      \n",
    "    val weightsBuilder: DataQuanta[Array[Double]] =\n",
    "      planBuilder.loadCollection(weights).withName(\"init weights\")\n",
    "      \n",
    "    // Load and transform the data.\n",
    "    val transformBuilder: DataQuanta[Array[Double]] = planBuilder\n",
    "      .readTextFile(datasetUrl).withName(\"source\")\n",
    "      .mapJava(new Transform(features)).withName(\"transform\")\n",
    "      \n",
    "    // Do the SGD\n",
    "    val loop: DataQuanta[Array[Double]] = weightsBuilder.doWhileJava(\n",
    "      new LoopCondition(accuracy, maxIterations),\n",
    "      (w) => {\n",
    "        var newWeightsDataset: DataQuanta[Array[Double]] =\n",
    "          transformBuilder\n",
    "            .sample(sampleSize, datasetSize).withBroadcast(w, \"weights\")\n",
    "            .mapJava(new ComputeLogisticGradient()).withBroadcast(w, \"weights\").withName(\"compute\")\n",
    "            .reduceJava(new Sum()).withName(\"reduce\")\n",
    "            .mapJava(new WeightsUpdate()).withBroadcast(w, \"weights\").withName(\"update\")\n",
    "          \n",
    "        var convergenceDataset: DataQuanta[(Double, Double)] = \n",
    "                 newWeightsDataset.mapJava(new ComputeNorm()).withBroadcast(w, \"weights\")\n",
    "          \n",
    "        new WayangTuple(newWeightsDataset, convergenceDataset)\n",
    "      },\n",
    "      maxIterations\n",
    "    )\n",
    "      \n",
    "    WayangCollections.getSingleOrNull(loop.collect())\n",
    "    \n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:19:55.362 [scala-interpreter-1] INFO  org.apache.wayang.core.api.Job - Preparing plan...\n",
      "15:19:55.491 [scala-interpreter-1] INFO  org.apache.wayang.core.api.Job - Estimating cardinalities and execution load...\n",
      "15:19:56.370 [scala-interpreter-1] WARN  org.apache.wayang.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[ammonite.$sess.cmd1$Helper$Transform@7fe82571].\n",
      "15:19:56.406 [scala-interpreter-1] WARN  org.apache.wayang.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[ammonite.$sess.cmd6$Helper$LoopCondition@3af98741].\n",
      "15:19:56.418 [scala-interpreter-1] WARN  org.apache.wayang.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[ammonite.$sess.cmd2$Helper$ComputeLogisticGradient@72328af6].\n",
      "15:19:56.428 [scala-interpreter-1] WARN  org.apache.wayang.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for ReduceDescriptor[ammonite.$sess.cmd3$Helper$Sum@3e0475a9].\n",
      "15:19:56.438 [scala-interpreter-1] WARN  org.apache.wayang.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[ammonite.$sess.cmd4$Helper$WeightsUpdate@628f990a].\n",
      "15:19:56.441 [scala-interpreter-1] WARN  org.apache.wayang.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[ammonite.$sess.cmd5$Helper$ComputeNorm@31507606].\n",
      "15:19:57.404 [scala-interpreter-1] INFO  org.apache.wayang.core.api.Job - Enumerating execution plans...\n",
      "15:19:57.456 [scala-interpreter-1] INFO  org.apache.wayang.core.optimizer.enumeration.PlanEnumeration - Concatenating 2*2=4 concatenation groups (out@Alternative[3x ~Alternative[[Sample[1+1->1, id=95aaf26]]], 5eb2b7e0] -> 1 inputs).\n",
      "15:19:57.597 [scala-interpreter-1] INFO  org.apache.wayang.core.optimizer.enumeration.PlanEnumeration - Concatenating 2*2=4 concatenation groups (out@Alternative[3x ~Alternative[[Map[update]]], 26aae14c] -> 1 inputs).\n",
      "15:19:57.661 [scala-interpreter-1] INFO  org.apache.wayang.core.optimizer.enumeration.PlanEnumeration - Concatenating 2*2=4 concatenation groups (out@Alternative[3x ~Alternative[[Map[compute]]], 343665a3] -> 1 inputs).\n",
      "15:19:57.720 [scala-interpreter-1] INFO  org.apache.wayang.core.optimizer.enumeration.PlanEnumeration - Concatenating 2*2=4 concatenation groups (out@Alternative[3x ~Alternative[[GlobalReduce[reduce]]], 44931811] -> 1 inputs).\n",
      "15:19:58.236 [scala-interpreter-1] INFO  org.apache.wayang.core.optimizer.enumeration.PlanEnumeration - Concatenating 16*2=32 concatenation groups (iterOut@LoopHeadAlternative[3x ~Alternative[[DoWhile[3->2, id=6fae39e4]]], 51d18ba2] -> 4 inputs).\n",
      "15:20:00.091 [scala-interpreter-1] WARN  org.apache.wayang.core.optimizer.enumeration.PlanImplementation - Replaced Junction[out@JavaMap[update]->[in@JavaMap[1+1->1, id=371dd2fe]]] with Junction[out@JavaMap[update]->[iterIn@JavaDoWhile[3->2, id=3e27289b], in@JavaMap[1+1->1, id=371dd2fe]]].\n",
      "15:20:00.116 [scala-interpreter-1] WARN  org.apache.wayang.core.optimizer.enumeration.PlanImplementation - Replaced Junction[out@JavaMap[update]->[in@JavaMap[1+1->1, id=371dd2fe]]] with Junction[out@JavaMap[update]->[iterIn@SparkDoWhile[3->2, id=41bac36f], in@JavaMap[1+1->1, id=371dd2fe]]].\n",
      "15:20:00.149 [scala-interpreter-1] WARN  org.apache.wayang.core.optimizer.enumeration.PlanImplementation - Replaced Junction[out@SparkMap[update]->[in@SparkMap[1+1->1, id=13abb323]]] with Junction[out@SparkMap[update]->[iterIn@JavaDoWhile[3->2, id=3e27289b], in@SparkMap[1+1->1, id=13abb323]]].\n",
      "15:20:00.172 [scala-interpreter-1] WARN  org.apache.wayang.core.optimizer.enumeration.PlanImplementation - Replaced Junction[out@SparkMap[update]->[in@SparkMap[1+1->1, id=13abb323]]] with Junction[out@SparkMap[update]->[iterIn@SparkDoWhile[3->2, id=41bac36f], in@SparkMap[1+1->1, id=13abb323]]].\n",
      "15:20:00.193 [scala-interpreter-1] WARN  org.apache.wayang.core.optimizer.enumeration.PlanImplementation - Replaced Junction[out@JavaMap[update]->[in@JavaMap[1+1->1, id=371dd2fe]]] with Junction[out@JavaMap[update]->[iterIn@JavaDoWhile[3->2, id=3e27289b], in@JavaMap[1+1->1, id=371dd2fe]]].\n",
      "15:20:00.214 [scala-interpreter-1] WARN  org.apache.wayang.core.optimizer.enumeration.PlanImplementation - Replaced Junction[out@SparkMap[update]->[in@SparkMap[1+1->1, id=13abb323]]] with Junction[out@SparkMap[update]->[iterIn@SparkDoWhile[3->2, id=41bac36f], in@SparkMap[1+1->1, id=13abb323]]].\n",
      "15:20:00.229 [scala-interpreter-1] INFO  org.apache.wayang.core.optimizer.enumeration.PlanEnumeration - Concatenating 2*2=4 concatenation groups (out@Alternative[3x ~Alternative[[TextFileSource[source]]], 69b3db8] -> 1 inputs).\n",
      "15:20:00.235 [scala-interpreter-1] INFO  org.apache.wayang.core.optimizer.enumeration.PlanEnumeration - Concatenating 2*2=4 concatenation groups (finOut@LoopSubplan[2->1, id=9fc2f33] -> 1 inputs).\n",
      "15:20:00.286 [scala-interpreter-1] INFO  org.apache.wayang.core.optimizer.enumeration.PlanEnumeration - Concatenating 2*2=4 concatenation groups (out@Alternative[3x ~Alternative[[Map[transform]]], 49cd087b] -> 1 inputs).\n",
      "15:20:00.296 [scala-interpreter-1] INFO  org.apache.wayang.core.optimizer.enumeration.PlanEnumeration - Concatenating 2*2=4 concatenation groups (out@Alternative[3x ~Alternative[[CollectionSource[init weights]]], 76ec4906] -> 1 inputs).\n",
      "15:20:00.302 [scala-interpreter-1] INFO  org.apache.wayang.core.api.Job - Picked PlanImplementation[[Platform[Apache Spark], Platform[Java Streams]], (0:07:47.109 .. 0:07:53.197, p=18.05%), costs=(467,109.00..473,197.00 ~ 18.1%)] as best plan.\n",
      "15:20:00.408 [scala-interpreter-1] INFO  org.apache.wayang.core.api.Job - Compiling execution plan...\n",
      "15:20:00.449 [scala-interpreter-1] INFO  org.apache.wayang.core.api.Job - Current execution plan:\n",
      ">>> ExecutionStage[T[JavaTextFileSource[source]]]:\n",
      ">     JavaTextFileSource[source] => StreamChannel => JavaMap[transform]\n",
      ">     JavaMap[transform] => StreamChannel => JavaCollect[convert out@JavaMap[transform]]\n",
      "> Out JavaCollect[convert out@JavaMap[transform]] => CollectionChannel\n",
      "\n",
      ">>> ExecutionStage[T[JavaCollectionSource[init weights]]]:\n",
      "> Out JavaCollectionSource[init weights] => CollectionChannel\n",
      "\n",
      ">>> ExecutionStage[T[SparkCollectionSource[convert out@JavaMap[transform]]]]:\n",
      "> In  CollectionChannel => SparkCollectionSource[convert out@JavaMap[transform]]\n",
      ">     SparkCollectionSource[convert out@JavaMap[transform]] => RddChannel => SparkCache[convert out@JavaMap[transform]]\n",
      "> Out SparkCache[convert out@JavaMap[transform]] => RddChannel\n",
      "\n",
      ">>> ExecutionStage[T[JavaDoWhile[3->2, id=3e27289b]]]:\n",
      "> In  CollectionChannel => JavaDoWhile[3->2, id=3e27289b]\n",
      "> In  CollectionChannel => JavaDoWhile[3->2, id=3e27289b]\n",
      "> In  CollectionChannel => JavaDoWhile[3->2, id=3e27289b]\n",
      "> Out JavaDoWhile[3->2, id=3e27289b] => StreamChannel\n",
      "> Out JavaDoWhile[3->2, id=3e27289b] => StreamChannel\n",
      "\n",
      ">>> ExecutionStage[T[SparkShufflePartitionSample[1+1->1, id=64395e77]]]:\n",
      "> In  RddChannel => SparkShufflePartitionSample[1+1->1, id=64395e77]\n",
      "> In  RddChannel => SparkShufflePartitionSample[1+1->1, id=64395e77]\n",
      "> Out SparkShufflePartitionSample[1+1->1, id=64395e77] => CollectionChannel\n",
      "\n",
      ">>> ExecutionStage[T[JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]]]:\n",
      "> In  StreamChannel => JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]\n",
      "> Out JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => CollectionChannel\n",
      "> Out JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => CollectionChannel\n",
      "> Out JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => CollectionChannel\n",
      "> Out JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => CollectionChannel\n",
      "\n",
      ">>> ExecutionStage[T[JavaLocalCallbackSink[collect()]]]:\n",
      "> In  StreamChannel => JavaLocalCallbackSink[collect()]\n",
      "\n",
      ">>> ExecutionStage[T[JavaMap[compute]]]:\n",
      "> In  CollectionChannel => JavaMap[compute]\n",
      "> In  CollectionChannel => JavaMap[compute]\n",
      ">     JavaMap[compute] => StreamChannel => JavaGlobalReduce[reduce]\n",
      "> Out JavaGlobalReduce[reduce] => CollectionChannel\n",
      "\n",
      ">>> ExecutionStage[T[JavaMap[1+1->1, id=371dd2fe]]]:\n",
      "> In  CollectionChannel => JavaMap[1+1->1, id=371dd2fe]\n",
      "> In  CollectionChannel => JavaMap[1+1->1, id=371dd2fe]\n",
      ">     JavaMap[1+1->1, id=371dd2fe] => StreamChannel => JavaCollect[convert out@JavaMap[1+1->1, id=371dd2fe]]\n",
      "> Out JavaCollect[convert out@JavaMap[1+1->1, id=371dd2fe]] => CollectionChannel\n",
      "\n",
      ">>> ExecutionStage[T[JavaMap[update]]]:\n",
      "> In  CollectionChannel => JavaMap[update]\n",
      "> In  CollectionChannel => JavaMap[update]\n",
      ">     JavaMap[update] => StreamChannel => JavaCollect[convert out@JavaMap[update]]\n",
      "> Out JavaCollect[convert out@JavaMap[update]] => CollectionChannel\n",
      "> Out JavaCollect[convert out@JavaMap[update]] => CollectionChannel\n",
      "\n",
      ">>> ExecutionStage[T[SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]]]:\n",
      "> In  CollectionChannel => SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]\n",
      "> Out SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => RddChannel\n",
      "\n",
      "\n",
      "15:20:00.463 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[JavaTextFileSource[source]]].\n",
      "15:20:00.463 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[JavaCollectionSource[init weights]]].\n",
      "15:20:00.470 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having JavaExecutor[0] execute ExecutionStage[T[JavaTextFileSource[source]]]:\n",
      ">     JavaTextFileSource[source] => StreamChannel => JavaMap[transform]\n",
      ">     JavaMap[transform] => StreamChannel => JavaCollect[convert out@JavaMap[transform]]\n",
      "> Out JavaCollect[convert out@JavaMap[transform]] => CollectionChannel\n",
      "15:20:00.480 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.002 (estimated (0:00:00.017 .. 0:00:00.019, p=85.50%)).\n",
      "15:20:03.025 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 3 items in 0:00:02.540 (estimated (0:00:00.030 .. 0:00:00.110, p=18.05%)).\n",
      "15:20:03.026 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[JavaTextFileSource[source]]] in 0:00:02.555 (2555 ms).\n",
      "15:20:03.027 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkCollectionSource[convert out@JavaMap[transform]]]].\n",
      "15:20:03.027 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having JavaExecutor[0] execute ExecutionStage[T[JavaCollectionSource[init weights]]]:\n",
      "> Out JavaCollectionSource[init weights] => CollectionChannel\n",
      "15:20:03.027 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.000 (estimated (0:00:00.001 .. 0:00:00.001, p=90.00%)).\n",
      "15:20:03.029 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[JavaCollectionSource[init weights]]] in 0:00:00.002 (2 ms).\n",
      "15:20:03.029 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[JavaDoWhile[3->2, id=3e27289b]]].\n",
      "15:20:03.029 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating next iteration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/03/04 15:20:03 INFO SparkContext: Running Spark version 3.1.2\n",
      "22/03/04 15:20:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/03/04 15:20:03 INFO ResourceUtils: ==============================================================\n",
      "22/03/04 15:20:03 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/03/04 15:20:03 INFO ResourceUtils: ==============================================================\n",
      "22/03/04 15:20:03 INFO SparkContext: Submitted application: Wayang app\n",
      "22/03/04 15:20:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/03/04 15:20:03 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/03/04 15:20:03 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/03/04 15:20:03 INFO SecurityManager: Changing view acls to: jovyan\n",
      "22/03/04 15:20:03 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "22/03/04 15:20:03 INFO SecurityManager: Changing view acls groups to: \n",
      "22/03/04 15:20:03 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/03/04 15:20:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "22/03/04 15:20:04 INFO Utils: Successfully started service 'sparkDriver' on port 43023.\n",
      "22/03/04 15:20:04 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/03/04 15:20:04 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/03/04 15:20:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/03/04 15:20:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/03/04 15:20:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/03/04 15:20:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-66e14c97-f93e-44ad-8c5d-cca500cab799\n",
      "22/03/04 15:20:04 INFO MemoryStore: MemoryStore started with capacity 880.5 MiB\n",
      "22/03/04 15:20:04 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/03/04 15:20:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/03/04 15:20:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3164d0df1529:4040\n",
      "22/03/04 15:20:04 INFO Executor: Starting executor ID driver on host 3164d0df1529\n",
      "22/03/04 15:20:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46189.\n",
      "22/03/04 15:20:04 INFO NettyBlockTransferService: Server created on 3164d0df1529:46189\n",
      "22/03/04 15:20:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/03/04 15:20:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3164d0df1529, 46189, None)\n",
      "22/03/04 15:20:04 INFO BlockManagerMasterEndpoint: Registering block manager 3164d0df1529:46189 with 880.5 MiB RAM, BlockManagerId(driver, 3164d0df1529, 46189, None)\n",
      "22/03/04 15:20:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3164d0df1529, 46189, None)\n",
      "22/03/04 15:20:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3164d0df1529, 46189, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:05.040 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having SparkExecutor[1] execute ExecutionStage[T[SparkCollectionSource[convert out@JavaMap[transform]]]]:\n",
      "> In  CollectionChannel => SparkCollectionSource[convert out@JavaMap[transform]]\n",
      ">     SparkCollectionSource[convert out@JavaMap[transform]] => RddChannel => SparkCache[convert out@JavaMap[transform]]\n",
      "> Out SparkCache[convert out@JavaMap[transform]] => RddChannel\n",
      "15:20:05.387 [scala-interpreter-1] WARN  org.apache.wayang.spark.execution.SparkExecutor - Execution of T[SparkCollectionSource[convert out@JavaMap[transform]]] took suspiciously long (0:00:00.345).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/04 15:20:05 INFO SparkContext: Starting job: foreachPartition at SparkCacheOperator.java:62\n",
      "22/03/04 15:20:05 INFO DAGScheduler: Got job 0 (foreachPartition at SparkCacheOperator.java:62) with 4 output partitions\n",
      "22/03/04 15:20:05 INFO DAGScheduler: Final stage: ResultStage 0 (foreachPartition at SparkCacheOperator.java:62)\n",
      "22/03/04 15:20:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/03/04 15:20:05 INFO DAGScheduler: Missing parents: List()\n",
      "22/03/04 15:20:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at filter at RddChannel.java:91), which has no missing parents\n",
      "22/03/04 15:20:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.6 KiB, free 880.5 MiB)\n",
      "22/03/04 15:20:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 880.5 MiB)\n",
      "22/03/04 15:20:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3164d0df1529:46189 (size: 2.4 KiB, free: 880.5 MiB)\n",
      "22/03/04 15:20:05 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1388\n",
      "22/03/04 15:20:05 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at filter at RddChannel.java:91) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "22/03/04 15:20:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks resource profile 0\n",
      "22/03/04 15:20:05 WARN TaskSetManager: Stage 0 contains a task of very large size (6503 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/04 15:20:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (3164d0df1529, executor driver, partition 0, PROCESS_LOCAL, 6659462 bytes) taskResourceAssignments Map()\n",
      "22/03/04 15:20:05 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (3164d0df1529, executor driver, partition 1, PROCESS_LOCAL, 6659462 bytes) taskResourceAssignments Map()\n",
      "22/03/04 15:20:05 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "22/03/04 15:20:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "22/03/04 15:20:06 INFO MemoryStore: Block rdd_1_0 stored as values in memory (estimated size 6.6 MiB, free 873.9 MiB)\n",
      "22/03/04 15:20:06 INFO BlockManagerInfo: Added rdd_1_0 in memory on 3164d0df1529:46189 (size: 6.6 MiB, free: 873.9 MiB)\n",
      "22/03/04 15:20:06 INFO MemoryStore: Block rdd_1_1 stored as values in memory (estimated size 6.6 MiB, free 867.3 MiB)\n",
      "22/03/04 15:20:06 INFO BlockManagerInfo: Added rdd_1_1 in memory on 3164d0df1529:46189 (size: 6.6 MiB, free: 867.3 MiB)\n",
      "22/03/04 15:20:06 INFO Executor: 1 block locks were not released by task 0.0 in stage 0.0 (TID 0)\n",
      "[rdd_1_0]\n",
      "22/03/04 15:20:06 INFO Executor: 1 block locks were not released by task 1.0 in stage 0.0 (TID 1)\n",
      "[rdd_1_1]\n",
      "22/03/04 15:20:06 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 966 bytes result sent to driver\n",
      "22/03/04 15:20:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 966 bytes result sent to driver\n",
      "22/03/04 15:20:06 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (3164d0df1529, executor driver, partition 2, PROCESS_LOCAL, 6659462 bytes) taskResourceAssignments Map()\n",
      "22/03/04 15:20:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1073 ms on 3164d0df1529 (executor driver) (1/4)\n",
      "22/03/04 15:20:06 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "22/03/04 15:20:06 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (3164d0df1529, executor driver, partition 3, PROCESS_LOCAL, 6659462 bytes) taskResourceAssignments Map()\n",
      "22/03/04 15:20:06 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1031 ms on 3164d0df1529 (executor driver) (2/4)\n",
      "22/03/04 15:20:06 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "22/03/04 15:20:06 INFO MemoryStore: Block rdd_1_2 stored as values in memory (estimated size 6.6 MiB, free 860.7 MiB)\n",
      "22/03/04 15:20:06 INFO BlockManagerInfo: Added rdd_1_2 in memory on 3164d0df1529:46189 (size: 6.6 MiB, free: 860.7 MiB)\n",
      "22/03/04 15:20:06 INFO Executor: 1 block locks were not released by task 2.0 in stage 0.0 (TID 2)\n",
      "[rdd_1_2]\n",
      "22/03/04 15:20:06 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 880 bytes result sent to driver\n",
      "22/03/04 15:20:06 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 154 ms on 3164d0df1529 (executor driver) (3/4)\n",
      "22/03/04 15:20:06 INFO MemoryStore: Block rdd_1_3 stored as values in memory (estimated size 6.6 MiB, free 854.1 MiB)\n",
      "22/03/04 15:20:06 INFO BlockManagerInfo: Added rdd_1_3 in memory on 3164d0df1529:46189 (size: 6.6 MiB, free: 854.1 MiB)\n",
      "22/03/04 15:20:07 INFO Executor: 1 block locks were not released by task 3.0 in stage 0.0 (TID 3)\n",
      "[rdd_1_3]\n",
      "22/03/04 15:20:07 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 880 bytes result sent to driver\n",
      "22/03/04 15:20:07 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 140 ms on 3164d0df1529 (executor driver) (4/4)\n",
      "22/03/04 15:20:07 INFO DAGScheduler: ResultStage 0 (foreachPartition at SparkCacheOperator.java:62) finished in 1.453 s\n",
      "22/03/04 15:20:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "22/03/04 15:20:07 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/03/04 15:20:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Job 0 finished: foreachPartition at SparkCacheOperator.java:62, took 1.538000 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:07.034 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 2 items in 0:00:01.645 (estimated (0:00:04.859 .. 0:00:04.898, p=81.23%)).\n",
      "15:20:07.034 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkCollectionSource[convert out@JavaMap[transform]]]] in 0:00:01.993 (1993 ms).\n",
      "15:20:07.035 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having JavaExecutor[0] execute ExecutionStage[T[JavaDoWhile[3->2, id=3e27289b]]]:\n",
      "> In  CollectionChannel => JavaDoWhile[3->2, id=3e27289b]\n",
      "> In  CollectionChannel => JavaDoWhile[3->2, id=3e27289b]\n",
      "> In  CollectionChannel => JavaDoWhile[3->2, id=3e27289b]\n",
      "> Out JavaDoWhile[3->2, id=3e27289b] => StreamChannel\n",
      "> Out JavaDoWhile[3->2, id=3e27289b] => StreamChannel\n",
      "15:20:07.036 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.001 (estimated (0:00:00.002 .. 0:00:00.002, p=20.00%)).\n",
      "15:20:07.036 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[JavaDoWhile[3->2, id=3e27289b]]] in 0:00:00.001 (1 ms).\n",
      "15:20:07.037 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]]].\n",
      "15:20:07.037 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having JavaExecutor[0] execute ExecutionStage[T[JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]]]:\n",
      "> In  StreamChannel => JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]\n",
      "> Out JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => CollectionChannel\n",
      "> Out JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => CollectionChannel\n",
      "> Out JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => CollectionChannel\n",
      "> Out JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => CollectionChannel\n",
      "15:20:07.040 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.000 (estimated (0:00:00.001 .. 0:00:00.001, p=90.00%)).\n",
      "15:20:07.042 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]]] in 0:00:00.001 (1 ms).\n",
      "15:20:07.042 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]]].\n",
      "15:20:07.043 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having SparkExecutor[1] execute ExecutionStage[T[SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]]]:\n",
      "> In  CollectionChannel => SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]\n",
      "> Out SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => RddChannel\n",
      "15:20:07.049 [scala-interpreter-1] INFO  org.apache.wayang.spark.execution.SparkExecutor - T[SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]] was not executed eagerly as requested.\n",
      "15:20:07.049 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]]] in 0:00:00.006 (6 ms).\n",
      "15:20:07.050 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkShufflePartitionSample[1+1->1, id=64395e77]]].\n",
      "15:20:07.052 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having SparkExecutor[1] execute ExecutionStage[T[SparkShufflePartitionSample[1+1->1, id=64395e77]]]:\n",
      "> In  RddChannel => SparkShufflePartitionSample[1+1->1, id=64395e77]\n",
      "> In  RddChannel => SparkShufflePartitionSample[1+1->1, id=64395e77]\n",
      "> Out SparkShufflePartitionSample[1+1->1, id=64395e77] => CollectionChannel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/04 15:20:07 INFO SparkContext: Starting job: runJob at SparkShufflePartitionSampleOperator.java:126\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Got job 1 (runJob at SparkShufflePartitionSampleOperator.java:126) with 1 output partitions\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at SparkShufflePartitionSampleOperator.java:126)\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Missing parents: List()\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at mapPartitionsWithIndex at SparkShufflePartitionSampleOperator.java:120), which has no missing parents\n",
      "22/03/04 15:20:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.8 KiB, free 854.1 MiB)\n",
      "22/03/04 15:20:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 854.1 MiB)\n",
      "22/03/04 15:20:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3164d0df1529:46189 (size: 2.5 KiB, free: 854.1 MiB)\n",
      "22/03/04 15:20:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1388\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at mapPartitionsWithIndex at SparkShufflePartitionSampleOperator.java:120) (first 15 tasks are for partitions Vector(3))\n",
      "22/03/04 15:20:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "22/03/04 15:20:07 WARN TaskSetManager: Stage 1 contains a task of very large size (6503 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/04 15:20:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4) (3164d0df1529, executor driver, partition 3, PROCESS_LOCAL, 6659462 bytes) taskResourceAssignments Map()\n",
      "22/03/04 15:20:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 4)\n",
      "22/03/04 15:20:07 INFO BlockManager: Found block rdd_1_3 locally\n",
      "22/03/04 15:20:07 INFO MemoryStore: Block rdd_4_3 stored as values in memory (estimated size 6.6 MiB, free 847.4 MiB)\n",
      "22/03/04 15:20:07 INFO BlockManagerInfo: Added rdd_4_3 in memory on 3164d0df1529:46189 (size: 6.6 MiB, free: 847.5 MiB)\n",
      "22/03/04 15:20:07 INFO Executor: 1 block locks were not released by task 0.0 in stage 1.0 (TID 4)\n",
      "[rdd_4_3]\n",
      "22/03/04 15:20:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 4). 1238 bytes result sent to driver\n",
      "22/03/04 15:20:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 125 ms on 3164d0df1529 (executor driver) (1/1)\n",
      "22/03/04 15:20:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "22/03/04 15:20:07 INFO DAGScheduler: ResultStage 1 (runJob at SparkShufflePartitionSampleOperator.java:126) finished in 0.149 s\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/03/04 15:20:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Job 1 finished: runJob at SparkShufflePartitionSampleOperator.java:126, took 0.165493 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:07.246 [scala-interpreter-1] WARN  org.apache.wayang.spark.execution.SparkExecutor - Execution of T[SparkShufflePartitionSample[1+1->1, id=64395e77]] took suspiciously long (0:00:00.193).\n",
      "15:20:07.246 [scala-interpreter-1] INFO  org.apache.wayang.spark.execution.SparkExecutor - T[SparkShufflePartitionSample[1+1->1, id=64395e77]] was not executed eagerly as requested.\n",
      "15:20:07.246 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkShufflePartitionSample[1+1->1, id=64395e77]]] in 0:00:00.193 (193 ms).\n",
      "15:20:07.246 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[JavaMap[compute]]].\n",
      "15:20:07.246 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having JavaExecutor[0] execute ExecutionStage[T[JavaMap[compute]]]:\n",
      "> In  CollectionChannel => JavaMap[compute]\n",
      "> In  CollectionChannel => JavaMap[compute]\n",
      ">     JavaMap[compute] => StreamChannel => JavaGlobalReduce[reduce]\n",
      "> Out JavaGlobalReduce[reduce] => CollectionChannel\n",
      "15:20:07.248 [scala-interpreter-1] WARN  org.apache.wayang.java.execution.JavaExecutor - No cardinality available for RddChannel[T[SparkCache[convert out@JavaMap[transform]]]->[T[SparkShufflePartitionSample[1+1->1, id=64395e77]]]], although it was requested.\n",
      "15:20:07.249 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 4 items in 0:00:00.001 (estimated (0:00:00.462 .. 0:00:00.466, p=20.00%)).\n",
      "15:20:07.252 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[JavaMap[compute]]] in 0:00:00.005 (5 ms).\n",
      "15:20:07.252 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[JavaMap[update]]].\n",
      "15:20:07.253 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having JavaExecutor[0] execute ExecutionStage[T[JavaMap[update]]]:\n",
      "> In  CollectionChannel => JavaMap[update]\n",
      "> In  CollectionChannel => JavaMap[update]\n",
      ">     JavaMap[update] => StreamChannel => JavaCollect[convert out@JavaMap[update]]\n",
      "> Out JavaCollect[convert out@JavaMap[update]] => CollectionChannel\n",
      "> Out JavaCollect[convert out@JavaMap[update]] => CollectionChannel\n",
      "15:20:07.261 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 2 items in 0:00:00.005 (estimated (0:00:00.003 .. 0:00:00.003, p=20.00%)).\n",
      "15:20:07.263 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[JavaMap[update]]] in 0:00:00.010 (10 ms).\n",
      "15:20:07.263 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[JavaMap[1+1->1, id=371dd2fe]]].\n",
      "15:20:07.265 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having JavaExecutor[0] execute ExecutionStage[T[JavaMap[1+1->1, id=371dd2fe]]]:\n",
      "> In  CollectionChannel => JavaMap[1+1->1, id=371dd2fe]\n",
      "> In  CollectionChannel => JavaMap[1+1->1, id=371dd2fe]\n",
      ">     JavaMap[1+1->1, id=371dd2fe] => StreamChannel => JavaCollect[convert out@JavaMap[1+1->1, id=371dd2fe]]\n",
      "> Out JavaCollect[convert out@JavaMap[1+1->1, id=371dd2fe]] => CollectionChannel\n",
      "15:20:07.266 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 2 items in 0:00:00.001 (estimated (0:00:00.003 .. 0:00:00.003, p=20.00%)).\n",
      "15:20:07.267 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[JavaMap[1+1->1, id=371dd2fe]]] in 0:00:00.002 (2 ms).\n",
      "15:20:07.267 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[JavaDoWhile[3->2, id=3e27289b]]].\n",
      "15:20:07.267 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating next iteration.\n",
      "15:20:07.272 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having JavaExecutor[0] execute ExecutionStage[T[JavaDoWhile[3->2, id=3e27289b]]]:\n",
      "> In  CollectionChannel => JavaDoWhile[3->2, id=3e27289b]\n",
      "> In  CollectionChannel => JavaDoWhile[3->2, id=3e27289b]\n",
      "> In  CollectionChannel => JavaDoWhile[3->2, id=3e27289b]\n",
      "> Out JavaDoWhile[3->2, id=3e27289b] => StreamChannel\n",
      "> Out JavaDoWhile[3->2, id=3e27289b] => StreamChannel\n",
      "Running iteration: 1\n",
      "15:20:07.273 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.000 (estimated (0:00:00.002 .. 0:00:00.002, p=20.00%)).\n",
      "15:20:07.273 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[JavaDoWhile[3->2, id=3e27289b]]] in 0:00:00.001 (1 ms).\n",
      "15:20:07.274 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]]].\n",
      "15:20:07.275 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having JavaExecutor[0] execute ExecutionStage[T[JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]]]:\n",
      "> In  StreamChannel => JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]\n",
      "> Out JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => CollectionChannel\n",
      "> Out JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => CollectionChannel\n",
      "> Out JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => CollectionChannel\n",
      "> Out JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => CollectionChannel\n",
      "15:20:07.276 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.000 (estimated (0:00:00.001 .. 0:00:00.001, p=90.00%)).\n",
      "15:20:07.276 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[JavaCollect[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]]] in 0:00:00.000 (0 ms).\n",
      "15:20:07.276 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]]].\n",
      "15:20:07.276 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having SparkExecutor[1] execute ExecutionStage[T[SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]]]:\n",
      "> In  CollectionChannel => SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]\n",
      "> Out SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]] => RddChannel\n",
      "15:20:07.279 [scala-interpreter-1] INFO  org.apache.wayang.spark.execution.SparkExecutor - T[SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]] was not executed eagerly as requested.\n",
      "15:20:07.279 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkCollectionSource[convert iterOut@JavaDoWhile[3->2, id=3e27289b]]]] in 0:00:00.002 (2 ms).\n",
      "15:20:07.279 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkShufflePartitionSample[1+1->1, id=64395e77]]].\n",
      "15:20:07.280 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having SparkExecutor[1] execute ExecutionStage[T[SparkShufflePartitionSample[1+1->1, id=64395e77]]]:\n",
      "> In  RddChannel => SparkShufflePartitionSample[1+1->1, id=64395e77]\n",
      "> In  RddChannel => SparkShufflePartitionSample[1+1->1, id=64395e77]\n",
      "> Out SparkShufflePartitionSample[1+1->1, id=64395e77] => CollectionChannel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/04 15:20:07 INFO SparkContext: Starting job: runJob at SparkShufflePartitionSampleOperator.java:126\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Got job 2 (runJob at SparkShufflePartitionSampleOperator.java:126) with 1 output partitions\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at SparkShufflePartitionSampleOperator.java:126)\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Missing parents: List()\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[4] at mapPartitionsWithIndex at SparkShufflePartitionSampleOperator.java:120), which has no missing parents\n",
      "22/03/04 15:20:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.8 KiB, free 847.4 MiB)\n",
      "22/03/04 15:20:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 847.4 MiB)\n",
      "22/03/04 15:20:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3164d0df1529:46189 (size: 2.5 KiB, free: 847.4 MiB)\n",
      "22/03/04 15:20:07 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1388\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[4] at mapPartitionsWithIndex at SparkShufflePartitionSampleOperator.java:120) (first 15 tasks are for partitions Vector(3))\n",
      "22/03/04 15:20:07 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "22/03/04 15:20:07 WARN TaskSetManager: Stage 2 contains a task of very large size (6503 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/04 15:20:07 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5) (3164d0df1529, executor driver, partition 3, PROCESS_LOCAL, 6659462 bytes) taskResourceAssignments Map()\n",
      "22/03/04 15:20:07 INFO Executor: Running task 0.0 in stage 2.0 (TID 5)\n",
      "22/03/04 15:20:07 INFO BlockManager: Found block rdd_4_3 locally\n",
      "22/03/04 15:20:07 INFO Executor: 1 block locks were not released by task 0.0 in stage 2.0 (TID 5)\n",
      "[rdd_4_3]\n",
      "22/03/04 15:20:07 INFO Executor: Finished task 0.0 in stage 2.0 (TID 5). 1238 bytes result sent to driver\n",
      "22/03/04 15:20:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 57 ms on 3164d0df1529 (executor driver) (1/1)\n",
      "22/03/04 15:20:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "22/03/04 15:20:07 INFO DAGScheduler: ResultStage 2 (runJob at SparkShufflePartitionSampleOperator.java:126) finished in 0.068 s\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/03/04 15:20:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "22/03/04 15:20:07 INFO DAGScheduler: Job 2 finished: runJob at SparkShufflePartitionSampleOperator.java:126, took 0.073361 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:07.372 [scala-interpreter-1] WARN  org.apache.wayang.spark.execution.SparkExecutor - Execution of T[SparkShufflePartitionSample[1+1->1, id=64395e77]] took suspiciously long (0:00:00.092).\n",
      "15:20:07.372 [scala-interpreter-1] INFO  org.apache.wayang.spark.execution.SparkExecutor - T[SparkShufflePartitionSample[1+1->1, id=64395e77]] was not executed eagerly as requested.\n",
      "15:20:07.373 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkShufflePartitionSample[1+1->1, id=64395e77]]] in 0:00:00.093 (93 ms).\n",
      "15:20:07.373 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[JavaMap[compute]]].\n",
      "15:20:07.373 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having JavaExecutor[0] execute ExecutionStage[T[JavaMap[compute]]]:\n",
      "> In  CollectionChannel => JavaMap[compute]\n",
      "> In  CollectionChannel => JavaMap[compute]\n",
      ">     JavaMap[compute] => StreamChannel => JavaGlobalReduce[reduce]\n",
      "> Out JavaGlobalReduce[reduce] => CollectionChannel\n",
      "15:20:07.374 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 4 items in 0:00:00.000 (estimated (0:00:00.462 .. 0:00:00.466, p=20.00%)).\n",
      "15:20:07.374 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[JavaMap[compute]]] in 0:00:00.001 (1 ms).\n",
      "15:20:07.374 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[JavaMap[update]]].\n",
      "15:20:07.374 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having JavaExecutor[0] execute ExecutionStage[T[JavaMap[update]]]:\n",
      "> In  CollectionChannel => JavaMap[update]\n",
      "> In  CollectionChannel => JavaMap[update]\n",
      ">     JavaMap[update] => StreamChannel => JavaCollect[convert out@JavaMap[update]]\n",
      "> Out JavaCollect[convert out@JavaMap[update]] => CollectionChannel\n",
      "> Out JavaCollect[convert out@JavaMap[update]] => CollectionChannel\n",
      "15:20:07.375 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 2 items in 0:00:00.000 (estimated (0:00:00.003 .. 0:00:00.003, p=20.00%)).\n",
      "15:20:07.376 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[JavaMap[update]]] in 0:00:00.002 (2 ms).\n",
      "15:20:07.376 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[JavaMap[1+1->1, id=371dd2fe]]].\n",
      "15:20:07.376 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having JavaExecutor[0] execute ExecutionStage[T[JavaMap[1+1->1, id=371dd2fe]]]:\n",
      "> In  CollectionChannel => JavaMap[1+1->1, id=371dd2fe]\n",
      "> In  CollectionChannel => JavaMap[1+1->1, id=371dd2fe]\n",
      ">     JavaMap[1+1->1, id=371dd2fe] => StreamChannel => JavaCollect[convert out@JavaMap[1+1->1, id=371dd2fe]]\n",
      "> Out JavaCollect[convert out@JavaMap[1+1->1, id=371dd2fe]] => CollectionChannel\n",
      "15:20:07.377 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 2 items in 0:00:00.000 (estimated (0:00:00.003 .. 0:00:00.003, p=20.00%)).\n",
      "15:20:07.377 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[JavaMap[1+1->1, id=371dd2fe]]] in 0:00:00.001 (1 ms).\n",
      "15:20:07.377 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[JavaDoWhile[3->2, id=3e27289b]]].\n",
      "15:20:07.377 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating next iteration.\n",
      "15:20:07.377 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having JavaExecutor[0] execute ExecutionStage[T[JavaDoWhile[3->2, id=3e27289b]]]:\n",
      "> In  CollectionChannel => JavaDoWhile[3->2, id=3e27289b]\n",
      "> In  CollectionChannel => JavaDoWhile[3->2, id=3e27289b]\n",
      "> In  CollectionChannel => JavaDoWhile[3->2, id=3e27289b]\n",
      "> Out JavaDoWhile[3->2, id=3e27289b] => StreamChannel\n",
      "> Out JavaDoWhile[3->2, id=3e27289b] => StreamChannel\n",
      "Running iteration: 2\n",
      "15:20:07.378 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.000 (estimated (0:00:00.002 .. 0:00:00.002, p=20.00%)).\n",
      "15:20:07.379 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[JavaDoWhile[3->2, id=3e27289b]]] in 0:00:00.000 (0 ms).\n",
      "15:20:07.379 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[JavaLocalCallbackSink[collect()]]].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/04 15:20:07 INFO MapPartitionsRDD: Removing RDD 1 from persistence list\n",
      "22/03/04 15:20:07 INFO BlockManager: Removing RDD 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:07.427 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Having JavaExecutor[0] execute ExecutionStage[T[JavaLocalCallbackSink[collect()]]]:\n",
      "> In  StreamChannel => JavaLocalCallbackSink[collect()]\n",
      "15:20:07.428 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.000 (estimated (0:00:00.001 .. 0:00:00.001, p=90.00%)).\n",
      "15:20:07.428 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[JavaLocalCallbackSink[collect()]]] in 0:00:00.000 (0 ms).\n",
      "15:20:07.431 [scala-interpreter-1] INFO  org.apache.wayang.core.platform.CrossPlatformExecutor - Executed 11 stages in 0:00:06.968 (6968 ms).\n",
      "15:20:07.431 [scala-interpreter-1] INFO  org.apache.wayang.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of JavaMap[transform]'s output 0 from (104,470..115,467, 95.00%) to (110,000..110,000, 100.00%).\n",
      "15:20:07.432 [scala-interpreter-1] INFO  org.apache.wayang.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of JavaDoWhile[3->2, id=3e27289b]'s output 0 from (1..1, 100.00%) to (0..0, 100.00%).\n",
      "15:20:07.433 [scala-interpreter-1] INFO  org.apache.wayang.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of JavaDoWhile[3->2, id=3e27289b]'s output 0 from (1..1, 100.00%) to (0..0, 100.00%).\n",
      "15:20:07.433 [scala-interpreter-1] INFO  org.apache.wayang.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of JavaDoWhile[3->2, id=3e27289b]'s output 1 from null to (1..1, 100.00%).\n",
      "15:20:07.581 [scala-interpreter-1] INFO  org.apache.wayang.core.profiling.CardinalityRepository - Storing cardinalities at /home/jovyan/.wayang/cardinalities.json.\n",
      "15:20:07.582 [scala-interpreter-1] WARN  org.apache.wayang.core.profiling.CardinalityRepository - Cardinality repository currently disabled.\n",
      "15:20:07.584 [scala-interpreter-1] INFO  org.apache.wayang.core.profiling.ExecutionLog - Curating execution log at /home/jovyan/.wayang/executions.json.\n",
      "15:20:07.625 [scala-interpreter-1] INFO  org.apache.wayang.core.api.Job - Accumulated execution time: 0:00:06.991 (6991 ms) (effective: 0:00:04.195 (4195 ms), overhead: 0:00:02.796 (2796 ms))\n",
      "15:20:07.627 [scala-interpreter-1] INFO  org.apache.wayang.core.api.Job - Estimated execution time (plan 1): (0:07:47.109 .. 0:07:53.197, p=18.05%)\n",
      "15:20:07.632 [scala-interpreter-1] INFO  org.apache.wayang.core.api.Job - Accumulated costs: 4,195.00 .. 4,195.00\n",
      "15:20:07.635 [scala-interpreter-1] INFO  org.apache.wayang.core.api.Job - Estimated costs (plan 1): (467,109.00..473,197.00 ~ 18.1%)\n",
      "15:20:07.639 [scala-interpreter-1] INFO  org.apache.wayang.core.api.Job - Plan metrics: 10 virtual operators, 20 execution operators, 10 alternatives, 1024 combinations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/04 15:20:07 INFO SparkUI: Stopped Spark web UI at http://3164d0df1529:4040\n",
      "22/03/04 15:20:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "22/03/04 15:20:07 INFO MemoryStore: MemoryStore cleared\n",
      "22/03/04 15:20:07 INFO BlockManager: BlockManager stopped\n",
      "22/03/04 15:20:07 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "22/03/04 15:20:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "22/03/04 15:20:07 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:07.723 [scala-interpreter-1] INFO  org.apache.wayang.core.api.Job - StopWatch results:\n",
      "* Optimization                            - 0:00:05.076\n",
      "  * Prepare                               - 0:00:00.117\n",
      "    * Prune&Isolate                       - 0:00:00.029\n",
      "    * Transformations                     - 0:00:00.088\n",
      "    * Sanity                              - 0:00:00.000\n",
      "  * Cardinality&Load Estimation           - 0:00:01.912\n",
      "    * Create OptimizationContext          - 0:00:00.083\n",
      "    * Create CardinalityEstimationManager - 0:00:00.001\n",
      "    * Push Estimation                     - 0:00:01.828\n",
      "      * Estimate source cardinalities     - 0:00:01.072\n",
      "  * Create Initial Execution Plan         - 0:00:03.031\n",
      "    * Enumerate                           - 0:00:02.887\n",
      "      * Concatenation                     - 0:00:01.583\n",
      "        * Channel Conversion              - 0:00:01.529\n",
      "      * Prune                             - 0:00:01.120\n",
      "    * Pick Best Plan                      - 0:00:00.107\n",
      "    * Split Stages                        - 0:00:00.020\n",
      "* Execution                               - 0:00:06.991\n",
      "  * Execution 0                           - 0:00:06.991\n",
      "    * Execute                             - 0:00:06.977\n",
      "* Post-processing                         - 0:00:00.285\n",
      "  * Log measurements                      - 0:00:00.210\n",
      "  * Release Resources                     - 0:00:00.075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36minputFile\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"file:/home/jovyan/work/files/HIGGS.csv\"\u001b[39m\n",
       "\u001b[36mconfFile\u001b[39m: \u001b[32mConfiguration\u001b[39m = Configuration[file:/home/jovyan/work/files/wayang_sgd.properties]\n",
       "\u001b[36mres8_2\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m0.1620724640411348\u001b[39m,\n",
       "  \u001b[32m-0.7954137979210483\u001b[39m,\n",
       "  \u001b[32m-0.8245163390929532\u001b[39m,\n",
       "  \u001b[32m0.7273849291047905\u001b[39m,\n",
       "  \u001b[32m0.08607948894141675\u001b[39m,\n",
       "  \u001b[32m0.23305201346711177\u001b[39m,\n",
       "  \u001b[32m0.2513306789955911\u001b[39m,\n",
       "  \u001b[32m-0.033432965293703265\u001b[39m,\n",
       "  \u001b[32m0.0\u001b[39m,\n",
       "  \u001b[32m0.43554001211901405\u001b[39m,\n",
       "  \u001b[32m0.06130203905905599\u001b[39m,\n",
       "  \u001b[32m0.6732049581344751\u001b[39m,\n",
       "  \u001b[32m1.109276154278117\u001b[39m,\n",
       "  \u001b[32m0.6464049914937237\u001b[39m,\n",
       "  \u001b[32m0.09814112102492911\u001b[39m,\n",
       "  \u001b[32m-0.5814978994038001\u001b[39m,\n",
       "  \u001b[32m1.2741122245788574\u001b[39m,\n",
       "  \u001b[32m0.2588913381706192\u001b[39m,\n",
       "  \u001b[32m0.6657249521807111\u001b[39m,\n",
       "  \u001b[32m-0.8592695153674501\u001b[39m,\n",
       "  \u001b[32m0.0\u001b[39m,\n",
       "  \u001b[32m0.43110926578632486\u001b[39m,\n",
       "  \u001b[32m0.4282697499077778\u001b[39m,\n",
       "  \u001b[32m0.4949573960856999\u001b[39m,\n",
       "  \u001b[32m0.5516440596082104\u001b[39m,\n",
       "  \u001b[32m0.4358288941642178\u001b[39m,\n",
       "  \u001b[32m0.44888065695173923\u001b[39m,\n",
       "  \u001b[32m0.4275738364598869\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputFile = new File(\"files/HIGGS.csv\").toURI().toString()\n",
    "val confFile = new Configuration(new File(\"files/wayang_sgd.properties\").toURI().toString())\n",
    "\n",
    "new SGDImpl(Array(Spark.basicPlugin, Java.basicPlugin)).apply(confFile, inputFile, 11000000, 28, 1000, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  },
  "license": "Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.See the NOTICE file distributed with this work for additional information regarding copyright ownership.The ASF licenses this file to you under the Apache License, Version 2.0(the \"License\"); you may not use this file except in compliance with the License.You may obtain a copy of the License at http: //www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}